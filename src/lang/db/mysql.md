# 事务
## InnoDB 引擎通过什么技术来保证事务的这四个特性的呢？
持久性是通过 redo log （重做日志）来保证的；
原子性是通过 undo log（回滚日志） 来保证的；
隔离性是通过 MVCC（多版本并发控制） 或锁机制来保证的；
一致性则是通过持久性+原子性+隔离性来保证；
## 隔离级别
- 读未提交（Read uncommitted）：一个事务可以读取另一个事务未提交的数据。
- 读已提交（Read committed）：一个事务只能读取另一个事务已经提交的数据。
- **可重复读**（Repeatable read）：一个事务在执行过程中，同样的查询语句，每次返回的结果都是一样的。
- 串行化（Serializable）：事务串行执行，事务只能一个

在「可重复读」隔离级别下，可能发生幻读现象
_引发的问题_
  不可重复读的重点是事务内读取同一行数据的结果不一致，而幻读的重点是事务内多次查询得到的**结果集**不一致。

## 并行事务会引发什么问题
1. 脏读 如果一个事务「读到」了另一个「未提交事务修改过的数据」，就意味着发生了「脏读」现象。
2. 不可重复读
在一个事务内多次读取同一个数据，如果出现前后两次读到的数据不一样的情况，就意味着发生了「不可重复读」现象。
3. 幻读
在一个事务内多次查询某个符合查询条件的「记录数量」，如果出现前后两次查询到的记录数量不一样的情况，就意味着发生了「幻读」现象。

## 事务的实现方式
锁 ，无锁
1. 表锁，行锁
   乐观锁：update A set Name=lisi,version=version+1 where ID=#{id} and version=#{version}
   悲观锁
   Gap
   共享锁（S）：SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE
   排他锁（X）：SELECT * FROM table_name WHERE ... FOR UPDATE

2. MVCC 非锁定读
MVCC 当前读，快照读（加锁）
- 在InnoDB中，MVCC实现主要通过隐藏列（DATA_TRX_ID和DATA_ROLL_PTR）和undo log实现。当事务回滚时，undo log用于将数据恢复到修改前的样子；当读取记录时，若该记录被其他事务占用或当前版本对该事务不可见，则可以通过undo log读取之前的版本数据，以此实现非锁定读。
- Repeatable read配合gap锁不会出现幻读
```sql
Select * from emp where empid > 100 for update;
//InnoDB不仅会对符合条件的empid值为101的记录加锁，也会对empid大于101（这些记录并不存在）的“间隙”加锁
```

1. 锁定读

```sql
SELECT … FOR UPDATE
SELECT … LOCK IN SHARE MODE
```
4. AUTO-INC Locking
如果多个线程同时尝试插入记录，那么该计数器就会存在**竞态**，因此需要锁来确保自增过程的原子性，这种锁被称为AUTO-INC Locking 。
MySQL 5.1.22版本引入了一种轻量级互斥量的自增长实现机制，说人话就是采用CAS+自旋替代原有的互斥锁实现。
- 对于可以预先知道插入行数的插入操作而言，都采用CAS+自旋的方式实现，注意: 如果可以知道某次批量插入操作需要插入的行数，那么可以一次性申请n个自增长id ，然后事务自己后面再慢慢进行分配
- 对于无法提前获知插入行数的批量插入操作来说，采用互斥锁的方式实现，因为此过程可能会比较漫长，如果采用CAS+自旋可能会导致长时间的空自旋，浪费CPU资源
   
## 可重复读是如何工作的？
1. 假设事务 A （事务 id 为51）启动后，紧接着事务 B （事务 id 为52）也启动了，那这两个事务创建的 Read View 如下：
   trx_id,m_ids,min_trx_id,max_trx_id
2. 接着，事务 A 通过 update 语句将这条记录修改了（还未提交事务），将小林的余额改成 200 万，这时 MySQL 会记录相应的 undo log，并以链表的方式串联起来，形成版本链，如下图：
3. 然后事务 B 第二次去读取该记录，发现这条记录的 trx_id 值为 51，在事务 B 的 Read View 的 min_trx_id 和 max_trx_id 之间，则需要判断 trx_id 值是否在 m_ids 范围内，判断的结果是在的，那么说明这条记录是被还未提交的事务修改的，这时事务 B 并不会读取这个版本的记录。而是沿着 undo log 链条往下找旧版本的记录，直到找到 trx_id 「小于」事务 B 的 Read View 中的 min_trx_id 值的第一条记录，所以事务 B 能读取到的是 trx_id 为 50 的记录，也就是小林余额是 100 万的这条记录。

## MySQL 可重复读隔离级别，完全解决幻读了吗？


1. 针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。
2. 针对当前读（select ... for update 等语句），是通过 next-key lock（**记录锁+间隙锁**）方式解决了幻读，因为当执行 select ... for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。
这两个解决方案是很大程度上解决了幻读现象，但是还是有个别的情况造成的幻读现象是无法解决的。

# 锁

## 全局锁
全局锁应用场景是什么？

全局锁主要应用于做全库*逻辑备份*，这样在备份数据库期间，不会因为数据或表结构的更新，而出现备份文件的数据与预期的不一样。
## 表级锁
### 表锁
```
  //表级别的共享锁，也就是读锁；
lock tables t_student read;

//表级别的独占锁，也就是写锁；
lock tables t_stuent write;
```
### 元数据锁（MDL）;
  对一张表进行 CRUD 操作时，加的是 MDL 读锁；
对一张表做结构变更操作的时候，加的是 MDL 写锁；
这是因为申请 MDL 锁的操作会形成一个队列，队列中写锁获取优先级高于读锁，一旦出现 MDL 写锁等待，会阻塞后续该表的所有 CRUD 操作。
### 意向锁；
接着，说说意向锁。

在使用 InnoDB 引擎的表里对某些记录加上「共享锁」之前，需要先在表级别加上一个「意向共享锁」；
在使用 InnoDB 引擎的表里对某些纪录加上「独占锁」之前，需要先在表级别加上一个「意向独占锁」；
也就是，当执行插入、更新、删除操作，需要先对表加上「意向独占锁」，然后对该记录加独占锁。

而普通的 select 是不会加行级锁的，普通的 select 语句是利用 MVCC 实现一致性读，是无锁的。

不过，select 也是可以对记录加共享锁和独占锁的，具体方式如下：
```sql
//先在表上加上意向共享锁，然后对读取的记录加共享锁
select ... lock in share mode;

//先表上加上意向独占锁，然后对读取的记录加独占锁
select ... for update;
````
表锁和行锁是满足读读共享、读写互斥、写写互斥的。

如果没有「意向锁」，那么加「独占表锁」时，就需要遍历表里所有记录，查看是否有记录存在独占锁，这样效率会很慢。

那么有了「意向锁」，由于在对记录加独占锁前，先会加上表级别的意向独占锁，那么在加「独占表锁」时，直接查该表是否有意向独占锁，如果有就意味着表里已经有记录被加了独占锁，这样就不用去遍历表里的记录。

### AUTO-INC 锁；

AUTO-INC 锁是特殊的表锁机制，锁不是再一个事务提交后才释放，而是再执行完插入语句后就会立即释放。

在插入数据时，会加一个表级别的 AUTO-INC 锁，
nnoDB 存储引擎提供了个 innodb_autoinc_lock_mode 的系统变量，是用来控制选择用 AUTO-INC 锁，还是轻量级的锁。

当 innodb_autoinc_lock_mode = 0，就采用 AUTO-INC 锁，语句执行结束后才释放锁；
当 innodb_autoinc_lock_mode = 2，就采用轻量级锁，申请自增主键后就释放锁，并不需要等语句执行后才释放。
当 innodb_autoinc_lock_mode = 1：
普通 insert 语句，自增锁在申请之后就马上释放；
类似 insert … select 这样的批量插入数据的语句，自增锁还是要等语句结束后才被释放；
所以，当 innodb_autoinc_lock_mode = 2 时，并且 binlog_format = row，既能提升并发性，又不会出现数据一致性问题。

## 行级锁
行级锁的类型主要有三类：

Record Lock，记录锁，也就是仅仅把一条记录锁上；
Gap Lock，间隙锁，锁定一个范围，但是不包含记录本身；
Gap Lock 称为间隙锁，只存在于可重复读隔离级别，目的是为了解决可重复读隔离级别下幻读的现象
Next-Key Lock：Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。
next-key lock 是包含间隙锁+记录锁的，如果一个事务获取了 X 型的 next-key lock，那么另外一个事务在获取相同范围的 X 型的 next-key lock 时，是会被阻塞的。
插入意向锁
一个事务在插入一条记录的时候，需要判断插入位置是否已被其他事务加了间隙锁（next-key lock 也包含间隙锁）。

如果有的话，插入操作就会发生阻塞，直到拥有间隙锁的那个事务提交为止（释放间隙锁的时刻），在此期间会生成一个插入意向锁，表明有事务想在某个区间插入新记录，但是现在处于等待状态。


# 索引
## 数据结构

## 为什么 MySQL InnoDB 选择 B+tree 作为索引的数据结构？
1、B+Tree vs B Tree

B+Tree 只在*叶子节点存储数据*，而 B 树 的非叶子节点也要存储数据，所以 B+Tree 的单个节点的数据量更小，在相同的磁盘 I/O 次数下，就能查询更多的节点。

另外，B+Tree 叶子节点采用的是*双链表连接，适合 MySQL 中常见的基于范围的顺序查找*，而 B 树无法做到这一点。

2、B+Tree vs 二叉树

对于有 N 个叶子节点的 B+Tree，其搜索复杂度为O(logdN)，其中 d 表示节点允许的最大子节点个数为 d 个。

在实际的应用当中， d 值是大于100的，这样就保证了，即使*数据达到千万级别时，B+Tree 的高度依然维持在 3~4 层左右*，也就是说一次数据查询操作只需要做 3~4 次的磁盘 I/O 操作就能查询到目标数据。

而二叉树的每个父节点的儿子节点个数只能是 2 个，意味着其搜索复杂度为 O(logN)，这已经比 B+Tree 高出不少，因此二叉树检索到目标数据所经历的磁盘 I/O 次数要更多。

3、B+Tree vs Hash

Hash 在做等值查询的时候效率贼快，搜索复杂度为 O(1)。

但是 Hash 表不适合做范围查询，它更适合做等值的查询，这也是 B+Tree 索引要比 Hash 表索引有着更广泛的适用场景的原因。

## 非聚簇索引
非聚簇索引不一定回表查询吗(覆盖索引)
```sql
 SELECT name FROM table WHERE name='guang19';
```
- 覆盖索引即需要查询的字段正好是索引的字段，那么直接根据该索引，就可以查到数据了，而无需回表查询。
## Buffer Pool 缓存什么？
Buffer Pool 除了缓存「索引页」和「数据页」，还包括了 Undo 页，插入缓存、自适应哈希索引、锁信息等等。
## 为什么需要 redo log ？
Buffer Pool 是提高了读写效率没错，但是问题来了，Buffer Pool 是基于内存的，而内存总是不可靠，万一断电重启，还没来得及落盘的脏页数据就会丢失。
# SQL
## MySQL 是怎么解决幻读的？

MySQL InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它很大程度上避免幻读现象（并不是完全解决了，详见这篇文章 (opens new window)），解决的方案有两种：

针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。
针对当前读（select ... for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读，因为当执行 select ... for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。
## 什么 SQL 语句会加行级锁？
普通的 select 语句是不会对记录加锁的（除了串行化隔离级别），因为它属于快照读，是通过 MVCC（多版本并发控制）实现的
除了上面这两条锁定读语句会加行级锁之外，update 和 delete 操作都会加行级锁，且锁的类型都是独占锁(X型锁)。
## SQL 执行流程
![mysql](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/mysql%E6%9F%A5%E8%AF%A2%E6%B5%81%E7%A8%8B.png)
1. 连接器：建立连接，管理连接、校验用户身份；
2. 查询缓存：查询语句如果命中查询缓存则直接返回，否则继续往下执行。MySQL 8.0 已删除该模块；
3. 解析 SQL，通过解析器对 SQL 查询语句进行词法分析、语法分析，然后构建语法树，方便后续模块读取表名、字段、语句类型；
4. 执行 SQL：执行 SQL 共有三个阶段：
   1. 预处理阶段：检查表或字段是否存在；将 select * 中的 * 符号扩展为表上的所有列。
   2. 优化阶段：基于查询成本的考虑， 选择查询成本最小的执行计划；
   3. 执行阶段：根据执行计划执行 SQL 查询语句，从存储引擎读取记录，返回给客户端；

### explain
#### Type
system：如果表使用的引擎对于表行数统计是精确的（如：MyISAM），且表中只有一行记录的情况下，访问方法是 system ，是 const 的一种特例。
const：表中最多只有一行匹配的记录，一次查询就可以找到，常用于使用主键或唯一索引的所有字段作为查询条件。
eq_ref：当连表查询时，前一张表的行在当前这张表中只有一行与之对应。是除了 system 与 const 之外最好的 join 方式，常用于使用主键或唯一索引的所有字段作为连表条件。
ref：使用普通索引作为查询条件，查询结果可能找到多个符合条件的行。
index_merge：当查询条件使用了多个索引时，表示开启了 Index Merge 优化，此时执行计划中的 key 列列出了使用到的索引。range：对索引列进行范围查询，执行计划中的 key 列表示哪个索引被使用了。
index：查询遍历了整棵索引树，与 ALL 类似，只不过扫描的是索引，而索引一般在内存中，速度更快。
ALL：全表扫描
#### Key

#### Extra（重要）
这列包含了 MySQL 解析查询的额外信息，通过这些信息，可以更准确的理解 MySQL 到底是如何执行查询的。常见的值如下：Using filesort：在排序时使用了外部的索引排序，没有用到表内索引进行排序。
Using temporary：MySQL 需要创建临时表来存储查询的结果，常见于 ORDER BY 和 GROUP BY。
Using index：表明查询使用了覆盖索引，不用回表，查询效率非常高。
Using index condition：表示查询优化器选择使用了索引条件下推这个特性。
Using where：表明查询使用了 WHERE 子句进行条件过滤。一般在没有使用到索引的时候会出现。
Using join buffer (Block Nested Loop)：连

表查询的方式，表示当被驱动表的没有使用索引的时候，MySQL 会先将驱动表读出来放到 join buffer 中，再遍历被驱动表与驱动表进行查询。

## Sql优化
1. 避免使用SELECT*
2. 分页优化
3. 尽量避免多表做join
4. 建议不要使用外键与级联
5. 选择合适的字段类型
6. 尽量用UNION ALL代替UNON
7. 批量操作
8. Show Profle分析SQL执行性能
9. 优化慢SQL
10. ”正确使用索引
```
11. 选择合适的字段创建索引
12. 被频繁更新的字段应该慎重建立…
13. 尽可能的考虑建立联合索引而不…
14. 注意避免冗余索引
15. 考虑在字符串类型的字段上使用…
16. 避免索引失效
17. 别除长期未使用的索引
```
！[sql](https://oss.javaguide.cn/javamianshizhibei/javamianshizhibei-sql-optimization.png)

### SQl Like优化
索引与优化like查询 
1. like %keyword 索引失效，使用全表扫描。但可以通过翻转函数+like前模糊查询+建立翻转函数索引=走翻转函数索引，不走全表扫描。 
2.  like keyword% 索引有效。 

3. like %keyword% 索引失效，也无法使用反向索引。

 使用下面的函数来进行模糊查询，如果出现的位置〉0，表示包含该字符串。 查询效率比like要高。 如果： table.field like ‘%AAA%’ 可以改为 **locate** (‘AAA’ , table.field) > 0 LOCATE(substr,str) POSITION(substr IN str) 返回子串substr在字符串str第一个出现的位置，如果substr不是在str里面，返回0。 使用instr select count(*) from table t where instr(t.column,’xx’)> 0 这种查询效果很好，速度很快。 
 
 2. 查询%xx的记录 select count(c.c_ply_no) as COUNT from Policy_Data_All c, Item_Data_All i where c.c_ply_no = i.c_ply_no and i.C_LCN_NO like ’%245′ 在执行的时候，执行计划显示，消耗值，io值，cpu值均非常大，原因是like后模糊查询导致索引失效，进行全表扫描 

在执行的时候，执行计划显示，消耗值，io值，cpu值均非常大，原因是like后模糊查询导致索引失效，进行全表扫描 解决方法：这种只有前模糊的sql可以改造如下写法 select count(c.c_ply_no) as COUNT from Policy_Data_All c, Item_Data_All i where c.c_ply_no = i.c_ply_no and reverse(i.C_LCN_NO) like reverse(‘%245′) 使用翻转函数+like前模糊查询+建立翻转函数索引=走**翻转函数索引**，不走全扫描。有效降低消耗值，io值，cpu值这三个指标，尤其是io值的降低。

### 如何优化 count(*)？
1. 近似值

执行 explain 命令效率是很高的，因为它并不会真正的去查询，下图中的 rows 字段值就是 explain 命令对表 t_order 记录的估算值。
！[](https://cdn.xiaolincoding.com//mysql/other/7590623443e8f225e5652109e6d9e3d2.png)

2. 额外表保存计数值
当我们在数据表插入一条记录的同时，将计数表中的计数字段 + 1。也就是说，在新增和删除操作时，我们需要额外维护这个计数表。
### Count（1）
count(字段) 的执行效率相比前面的 count(1)、 count(*)、 count(主键字段) 执行效率是最差的。
而且 MySQL 会对 count(*) 和 count(1) 有个优化，如果有多个二级索引的时候，优化器会使用key_len 最小的二级索引进行扫描。
### 索引失效有哪些？
1. 对索引使用左或者左右模糊匹配 //因为索引 B+ 树是按照「索引值」有序排列存储的，只能根据前缀进行比较
2. 对索引使用函数 //因为索引保存的是索引字段的原始值，而不是经过函数计算后的值，自然就没办法走索引了。
3. 对索引进行表达式计算
4. 对索引隐式类型转换//但是如果索引字段是整型类型，查询条件中的输入参数即使字符串，是不会导致索引失效，还是可以走索引扫描。

 MySQL 在遇到字符串和数字比较的时候，会自动把字符串转为数字，然后再进行比较。
5. 联合索引非最左匹配

# 一致性
## 两阶段提交
事务的提交过程有两个阶段，就是将 redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入binlog，具体如下：

prepare 阶段：将 XID（内部 XA 事务的 ID） 写入到 redo log，同时将 redo log 对应的事务状态设置为 prepare，然后将 redo log 持久化到磁盘（innodb_flush_log_at_trx_commit = 1 的作用）；

commit 阶段：把 XID 写入到 binlog，然后将 binlog 持久化到磁盘（sync_binlog = 1 的作用），接着调用引擎的提交事务接口，将 redo log 状态设置为 commit，此时该状态并不需要持久化到磁盘，只需要 write 到文件系统的 page cache 中就够了，因为只要 binlog 写磁盘成功，就算 redo log 的状态还是 prepare 也没有关系，一样会被认为事务已经执行成功；

## MySQL 磁盘 I/O 很高，有什么优化的方法
1. 设置组提交的两个参数： binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数，延迟 binlog 刷盘的时机，从而*减少 binlog 的刷盘次数*。这个方法是基于“额外的故意等待”来实现的，因此可能会增加语句的响应时间，但即使 MySQL 进程中途挂了，也没有丢失数据的风险，因为 binlog 早被写入到 page cache 了，只要系统没有宕机，缓存在 page cache 里的 binlog 就会被持久化到磁盘。
2. 将 sync_binlog 设置为大于 1 的值（比较常见是 100~1000），表示每次提交事务都 write，但累积 N 个事务后才 fsync，相当于延迟了 binlog 刷盘的时机。但是这样做的风险是，主机掉电时会丢 N 个事务的 binlog 日志。
3. 将 innodb_flush_log_at_trx_commit 设置为 2。表示每次事务提交时，都只是*缓存*在 redo log buffer 里的 redo log 写到 redo log 文件，注意写入到「 redo log 文件」并不意味着写入到了磁盘，因为操作系统的文件系统中有个 Page Cache，专门用来缓存文件数据的，所以写入「 redo log文件」意味着写入到了操作系统的文件缓存，然后交由操作系统控制持久化到磁盘的时机。但是这样做的风险是，主机掉电的时候会丢数据。